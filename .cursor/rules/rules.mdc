Visão geral

Backend: Django 5 + Django REST Framework (DRF) + Django Channels (tempo real) + Celery (jobs)

Frontend: React + Vite + Tailwind + shadcn/ui

DB: PostgreSQL 15/16 + pgvector (coluna vector p/ embeddings)

Multi-tenant: por linha (tenant_id em todas as tabelas) + permissões por escopo

IA: seu Qwen/Ollama local via n8n (MCP/HTTP)

Ingestão WhatsApp: Evolution API via WebSocket (cliente Python assíncrono)

Deploy: Railway (Django/React + Postgres). No Postgres do Railway: CREATE EXTENSION IF NOT EXISTS vector;

Estrutura de pastas (monorepo)
/evosense
  /backend
    manage.py
    evosense/settings.py
    evosense/asgi.py
    evosense/celery.py
    /apps
      /tenancy
      /authn
      /connections
      /messages
      /ai
      /billing
      /experiments
    /ingestion
      evolution_ws.py
    /common
      utils.py, permissions.py
    requirements.txt
  /frontend
    (React + Vite)
  docker-compose.yml
  README.md

Banco + pgvector
Extensão e índice

No Railway (ou migration inicial):

CREATE EXTENSION IF NOT EXISTS vector;
-- 768 dims (ajuste ao seu encoder)
ALTER TABLE messages_message ADD COLUMN embedding vector(768);
CREATE INDEX IF NOT EXISTS idx_message_embedding
  ON messages_message USING ivfflat (embedding vector_cosine) WITH (lists = 100);


Dica: se o encoder for Qwen-mini-embeddings (por ex.), ajuste 768/1024 conforme o modelo.

Modelos principais (Django)
# apps/tenancy/models.py
import uuid
from django.db import models

class Tenant(models.Model):
    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)
    name = models.CharField(max_length=160)
    plan = models.CharField(max_length=32, default="starter")  # starter|pro|scale|enterprise
    next_billing_date = models.DateField(null=True, blank=True)
    status = models.CharField(max_length=16, default="active")  # active|suspended
    created_at = models.DateTimeField(auto_now_add=True)

# apps/authn/models.py
from django.contrib.auth.models import AbstractUser
from django.db import models
from apps.tenancy.models import Tenant

class User(AbstractUser):
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name="users")
    role = models.CharField(max_length=16, default="operator")  # admin|operator

# apps/connections/models.py
from django.db import models
from apps.tenancy.models import Tenant

class EvolutionConnection(models.Model):
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name="connections")
    name = models.CharField(max_length=80)
    evo_ws_url = models.URLField()
    evo_token = models.CharField(max_length=255)  # mantenha criptografado/Hashicorp/Vault se possível
    is_active = models.BooleanField(default=True)
    created_at = models.DateTimeField(auto_now_add=True)

# apps/messages/models.py
from django.db import models
from django.contrib.postgres.indexes import GinIndex
from apps.tenancy.models import Tenant

class Message(models.Model):
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE, related_name="messages")
    connection = models.ForeignKey('connections.EvolutionConnection', on_delete=models.SET_NULL, null=True)
    chat_id = models.CharField(max_length=128, db_index=True)
    sender = models.CharField(max_length=64)    # hash do número/ID
    text = models.TextField()
    created_at = models.DateTimeField(db_index=True)
    # resultados IA
    sentiment = models.FloatField(null=True, blank=True)     # -1..1
    emotion = models.CharField(max_length=40, null=True, blank=True)
    satisfaction = models.IntegerField(null=True, blank=True) # 0..100
    tone = models.CharField(max_length=40, null=True, blank=True)
    summary = models.CharField(max_length=200, null=True, blank=True)
    # pgvector
    embedding = models.BinaryField(null=True, blank=True)  # DRF/Prisma vs vector: use adapter no DAO (ver abaixo)

    class Meta:
        indexes = [GinIndex(fields=['text'])]  # full-text fallback


Observação pgvector em Django: não há campo nativo “Vector” no core. Duas abordagens:

Usar SQL cru para INSERT/UPDATE da coluna vector, convertendo list[float] → vector.

Usar um custom field/adapter. A prática mais simples é manter embedding como list[float] em memória e escrever com cursor.execute("UPDATE ... SET embedding = %s::vector", [pgvector_str]).

DAO helper:

# apps/messages/dao.py
from django.db import connection

def write_embedding(message_id: int, emb: list[float]):
    vec = "[" + ",".join(f"{x:.6f}" for x in emb) + "]"
    with connection.cursor() as cur:
        cur.execute("UPDATE messages_message SET embedding = %s::vector WHERE id = %s", [vec, message_id])

def semantic_search(tenant_id, query_emb, limit=20):
    vec = "[" + ",".join(f"{x:.6f}" for x in query_emb) + "]"
    with connection.cursor() as cur:
        cur.execute("""
            SELECT id, text, sentiment, satisfaction
            FROM messages_message
            WHERE tenant_id = %s
            ORDER BY embedding <-> %s::vector
            LIMIT %s
        """, [str(tenant_id), vec, limit])
        rows = cur.fetchall()
    return rows

Experimentos de prompt/modelo
# apps/experiments/models.py
from django.db import models
from apps.messages.models import Message
from apps.tenancy.models import Tenant

class PromptTemplate(models.Model):
    version = models.CharField(max_length=64, unique=True)
    body = models.TextField()  # texto do prompt
    created_at = models.DateTimeField(auto_now_add=True)

class Inference(models.Model):
    tenant = models.ForeignKey(Tenant, on_delete=models.CASCADE)
    message = models.ForeignKey(Message, on_delete=models.CASCADE, related_name="inferences")
    model_name = models.CharField(max_length=64)         # qwen-X, ollama-qwen2...
    prompt_version = models.CharField(max_length=64)
    template_hash = models.CharField(max_length=64)
    latency_ms = models.IntegerField()
    sentiment = models.FloatField()
    emotion = models.CharField(max_length=40)
    satisfaction = models.IntegerField()
    is_shadow = models.BooleanField(default=False)
    run_id = models.CharField(max_length=64, db_index=True)  # experimento
    created_at = models.DateTimeField(auto_now_add=True)

API (DRF) — endpoints

POST /api/auth/login — JWT

GET /api/me

GET /api/tenants/:id/metrics — KPIs (médias, volumes, p95 latência)

GET /api/messages?tenant_id=&chat_id=&q= — paginação + busca full-text

POST /api/messages/semantic-search — recebe query, retorna top-K (usa pgvector)

GET /api/connections / POST /api/connections — gerenciar Evolution

POST /api/experiments/replay — dispara backfill assíncrono (Celery) com prompt_version

GET /api/prompts / POST /api/prompts — registrar versões de prompt

POST /api/ai/analyze — força análise (debug/admin)

POST /api/webhooks/stripe — billing

POST /api/webhooks/evolution — (opcional) fallback http

Exemplo View (semantic search):

# apps/messages/views.py
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework.permissions import IsAuthenticated
from apps.messages.dao import semantic_search
from apps.ai.embeddings import embed_text

class SemanticSearchView(APIView):
    permission_classes = [IsAuthenticated]
    def post(self, request):
        tenant_id = request.user.tenant_id
        query = request.data.get("query", "")
        emb = embed_text(query)  # chama seu encoder local via n8n/HTTP
        rows = semantic_search(tenant_id, emb, limit=20)
        return Response({"results": [
            {"id": r[0], "text": r[1], "sentiment": r[2], "satisfaction": r[3]}
            for r in rows
        ]})

Ingestão Evolution (WebSocket) — Python assíncrono
# ingestion/evolution_ws.py
import asyncio, json, websockets, datetime
from django.utils import timezone
from apps.messages.models import Message
from apps.ai.tasks import analyze_message_async

async def listen_connection(conn):
    headers = [("Authorization", f"Bearer {conn.evo_token}")]
    async with websockets.connect(conn.evo_ws_url, extra_headers=headers) as ws:
        async for raw in ws:
            evt = json.loads(raw)
            if evt.get("type") == "message":
                msg = Message.objects.create(
                    tenant=conn.tenant,
                    connection=conn,
                    chat_id=evt["chatId"],
                    sender=evt["from"],
                    text=evt["body"],
                    created_at=timezone.now()
                )
                analyze_message_async.delay(str(conn.tenant_id), msg.id)  # Celery

async def main():
    from apps.connections.models import EvolutionConnection
    conns = EvolutionConnection.objects.filter(is_active=True)
    await asyncio.gather(*(listen_connection(c) for c in conns))

# rode em um worker separados (daphne/asgi + esse loop em um process)

Celery (jobs IA e backfill)
# evosense/celery.py
import os
from celery import Celery
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'evosense.settings')
app = Celery('evosense')
app.config_from_object('django.conf:settings', namespace='CELERY')
app.autodiscover_tasks()

# apps/ai/tasks.py
import time, requests
from celery import shared_task
from django.conf import settings
from apps.messages.models import Message
from apps.messages.dao import write_embedding
from apps.experiments.models import Inference, PromptTemplate
from apps.ai.embeddings import embed_text  # seu cliente p/ Qwen/Ollama via n8n

@shared_task
def analyze_message_async(tenant_id, message_id, prompt_version=None, is_shadow=False, run_id="prod"):
    msg = Message.objects.get(id=message_id)
    template = PromptTemplate.objects.order_by('-created_at').first() if not prompt_version \
        else PromptTemplate.objects.get(version=prompt_version)

    payload = {
        "tenant_id": tenant_id,
        "message": msg.text,
        "context": {"chat_id": msg.chat_id, "sender": msg.sender},
        "prompt_version": template.version
    }
    t0 = time.time()
    r = requests.post(settings.N8N_AI_WEBHOOK, json=payload, timeout=3.0)
    r.raise_for_status()
    data = r.json()
    latency = int((time.time() - t0) * 1000)

    # persistir no Message (apenas campeão) e sempre na tabela de Inference
    if not is_shadow:
        msg.sentiment = data["sentiment"]
        msg.emotion = data["emotion"]
        msg.satisfaction = data["satisfaction"]
        msg.tone = data.get("tone")
        msg.summary = data.get("summary")
        msg.save(update_fields=["sentiment","emotion","satisfaction","tone","summary"])

    # embedding (opcional por plano)
    try:
        emb = embed_text(msg.text)
        write_embedding(msg.id, emb)
    except Exception:
        pass

    Inference.objects.create(
        tenant_id=tenant_id, message=msg, model_name=data.get("model","qwen-local"),
        prompt_version=template.version, template_hash="...", latency_ms=latency,
        sentiment=data["sentiment"], emotion=data["emotion"],
        satisfaction=data["satisfaction"], is_shadow=is_shadow, run_id=run_id
    )


Backfill (replay com outro prompt):

@shared_task
def replay_window(tenant_id, start_iso, end_iso, prompt_version, run_id):
    qs = Message.objects.filter(tenant_id=tenant_id, created_at__range=[start_iso, end_iso]).values_list("id", flat=True)
    for mid in qs:
        analyze_message_async.delay(tenant_id, mid, prompt_version=prompt_version, is_shadow=True, run_id=run_id)

Channels (tempo real)

ASGI (Daphne/Uvicorn), Channels + Redis (Railway tem Redis opcional).

Emita eventos “message_analyzed” para salas por tenant_id/chat_id.

routing.py

from django.urls import re_path
from .consumers import TenantConsumer

websocket_urlpatterns = [
    re_path(r'ws/tenant/(?P<tenant_id>[^/]+)/$', TenantConsumer.as_asgi()),
]


consumers.py

from channels.generic.websocket import AsyncJsonWebsocketConsumer

class TenantConsumer(AsyncJsonWebsocketConsumer):
    async def connect(self):
        self.tenant_id = self.scope['url_route']['kwargs']['tenant_id']
        self.group = f"tenant_{self.tenant_id}"
        await self.channel_layer.group_add(self.group, self.channel_name)
        await self.accept()

    async def disconnect(self, code):
        await self.channel_layer.group_discard(self.group, self.channel_name)

    async def message_analyzed(self, event):
        await self.send_json(event["payload"])


No analyze_message_async, ao finalizar:

from asgiref.sync import async_to_sync
from channels.layers import get_channel_layer
layer = get_channel_layer()
async_to_sync(layer.group_send)(f"tenant_{tenant_id}", {
  "type": "message_analyzed",
  "payload": {"message_id": msg.id, "sentiment": msg.sentiment, "satisfaction": msg.satisfaction}
})

Frontend (React)

Páginas: Login, Dashboard, Conversas, Conexões, Experimentos, Billing

Tempo real: conectar no ws://<backend>/ws/tenant/<tenant_id>/ e atualizar cards

Busca semântica: endpoint /api/messages/semantic-search

Ex. hook WS:

import { useEffect, useRef } from 'react';

export function useTenantWS(tenantId: string) {
  const ref = useRef<WebSocket | null>(null);
  useEffect(() => {
    const ws = new WebSocket(`${import.meta.env.VITE_WS_BASE}/ws/tenant/${tenantId}/`);
    ref.current = ws;
    ws.onmessage = (e) => {
      const evt = JSON.parse(e.data);
      // atualizar store com evt.payload
    };
    return () => ws.close();
  }, [tenantId]);
}

Docker Compose (local)
version: "3.9"
services:
  db:
    image: pgvector/pgvector:pg16
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: evosense
    ports: ["5432:5432"]
  redis:
    image: redis:7
    ports: ["6379:6379"]
  backend:
    build: ./backend
    env_file: ./backend/.env
    depends_on: [db, redis]
    ports: ["8000:8000"]
  frontend:
    build: ./frontend
    ports: ["5173:5173"]


No Railway, use serviço Postgres nativo (rodar CREATE EXTENSION vector; uma vez), + app Django (executar python manage.py migrate).

Boas práticas multi-tenant

QuerySet base sempre filtrando por request.user.tenant_id (permission classes/filters DRF).

Plan limits: middleware ou service central (ex.: “max connections por plano”).

Mascaramento de PII (regex) antes de enviar ao n8n/IA.

Audit log por tenant (alterações de conexão, plano, export).

Por que pgvector aqui ajuda?

Busca semântica (encontrar mensagens parecidas com uma reclamação específica).

Deduplicação (detectar repetição de conteúdo/situações).

Clustering (tópicos de insatisfação).

Detecção de “casos similares resolvidos” p/ sugerir respostas.

se quiser, te envio na sequência:

requirements.txt do backend (Django, DRF, Channels, Celery, psycopg2, websockets),

migrations iniciais (incluindo CREATE EXTENSION vector),

um seed com PromptTemplate default e um comando management replay_experiment pra rodar backfill por janela.

quer que eu já te entregue esses três arquivos prontos?

Você disse:
ajuste o prompt e me passe novamente
O ChatGPT disse:

perfeito — abaixo está o PROMPT MESTRE AJUSTADO para o seu stack final: Django + DRF + Channels + Celery (backend), React + Vite + Tailwind + shadcn (frontend), PostgreSQL + pgvector (Railway), ingestão Evolution API, IA via MCP (n8n/HTTP), multi-tenant SaaS, billing mensal e experimentos de prompt.
Cole esse prompt no Cursor (modo Architect/Initialize Project). Ele já pede scaffolding, modelos, endpoints, jobs e deploy.

PROMPT MESTRE — EVO SENSE (Django + React + Postgres + pgvector)
# 🧩 EVO SENSE — PROJECT INITIALIZATION PROMPT (Django + React)

Você é um **arquiteto fullstack sênior**. Gere um **monorepo produção-ready** para um SaaS multi-tenant chamado **EvoSense**, com:

- **Backend:** Django 5, Django REST Framework, Django Channels (WebSockets), Celery (jobs), Python 3.11+
- **Frontend:** React + TypeScript + Vite + TailwindCSS + shadcn/ui
- **Banco:** PostgreSQL (Railway) com **pgvector** para busca semântica
- **Infra:** Docker + Railway
- **Mensageria:** Evolution API via WebSocket (ingestão de mensagens WhatsApp)
- **IA:** via MCP (n8n HTTP webhook) **ou** HTTP direto para modelo local (Qwen/Ollama)
- **Multi-tenancy:** por linha (`tenant_id`) + RBAC (admin/operator)
- **Billing:** Stripe mensal (ciclos de 30 dias), planos por quantidade de números conectados
- **Experimentos:** versionamento de prompts e *shadow inference* (champion/challenger)
- **Observabilidade:** logs estruturados e métricas básicas

## 🎯 Objetivos
1) **Ingestão**: consumir mensagens da Evolution API (por tenant/connection), persistir, enviar à IA e armazenar resultados.
2) **IA**: classificar `sentiment` (-1..1), `emotion`, `satisfaction` (0..100), `tone`, `summary`.
3) **Busca semântica**: indexar embeddings em `pgvector` e expor endpoint de semantic search.
4) **Multi-tenant**: isolar dados por tenant; impor limites por plano (nº de connections, retenção, exports/dia).
5) **Billing**: Stripe (planos Starter/Pro/Scale/Enterprise), cron diário, suspensão ao falhar cobrança.
6) **Experimentos**: registrar `prompt_version`, rodar replay/backfill, shadow online e métricas comparativas.
7) **Frontend**: autenticação, dashboard de KPIs, lista de conversas, detalhe, conexões, billing, experimentos; realtime via Channels.

## 🧱 Estrutura esperada do monorepo
```
/evosense
  /backend
    manage.py
    evosense/settings.py
    evosense/asgi.py
    evosense/celery.py
    /apps
      /tenancy
      /authn
      /connections
      /messages
      /ai
      /billing
      /experiments
    /ingestion
      evolution_ws.py
    /common
      utils.py
      permissions.py
    requirements.txt
    Dockerfile
  /frontend
    src/...
    index.html
    package.json
    vite.config.ts
    Dockerfile
  /docs
    ARCHITECTURE.md
    DB_SCHEMA.md
    DEPLOYMENT.md
  /rules.md
  docker-compose.yml
  README.md
```

## 🔐 Autenticação & Multi-tenant
- Custom User (extensão de AbstractUser) com `tenant` (FK) e `role` (`admin`/`operator`).
- Todo endpoint DRF filtra por `request.user.tenant_id`. Forneça um `BaseTenantViewSet`/`TenantFilterBackend`.
- Permissões: `IsAuthenticated` + `IsTenantMember` + `RoleRequired`.

## 🗃️ Modelos (Django ORM)
Crie modelos e migrations:

**tenancy.Tenant**
- `id (UUID, PK)`, `name`, `plan (starter|pro|scale|enterprise)`, `next_billing_date (date)`, `status (active|suspended)`, `created_at`

**authn.User** (extends AbstractUser)
- `tenant (FK)`, `role (admin|operator)`

**connections.EvolutionConnection**
- `tenant (FK)`, `name`, `evo_ws_url`, `evo_token` (armazenar de forma segura), `is_active`, `created_at`

**messages.Message**
- `tenant (FK)`, `connection (FK)`, `chat_id`, `sender` (hash do número), `text`, `created_at`
- resultados IA: `sentiment (float -1..1)`, `emotion (str)`, `satisfaction (int 0..100)`, `tone (str)`, `summary (str)`
- `embedding` (coluna pgvector) — **implementar via SQL raw helper** (ver DAO)
- índices: `created_at`, `chat_id` e GIN em `text`

**experiments.PromptTemplate**
- `version (unique)`, `body (Text)`, `created_at`

**experiments.Inference**
- `tenant (FK)`, `message (FK)`, `model_name`, `prompt_version`, `template_hash`, `latency_ms`, `sentiment`, `emotion`, `satisfaction`, `is_shadow (bool)`, `run_id`, `created_at`

**billing.PaymentAccount**
- `tenant (FK)`, `stripe_customer_id`, `status (active|expired|pending)`

## 🗄️ pgvector
- Migration para `CREATE EXTENSION IF NOT EXISTS vector;`
- Adicionar coluna `embedding vector(768)` em `messages_message` (ajuste dimensões conforme seu encoder).
- Criar índice IVFFLAT: `USING ivfflat (embedding vector_cosine) WITH (lists = 100)`.
- DAO helpers:
  - `write_embedding(message_id, list[float])` → SQL `UPDATE ... SET embedding = %s::vector`
  - `semantic_search(tenant_id, query_emb, limit)` → `ORDER BY embedding <-> %s::vector`

## 📡 Ingestão Evolution (WebSocket)
- `ingestion/evolution_ws.py` com asyncio + websockets.
- Iniciar um listener por `EvolutionConnection` ativa (multi-tenant).
- Ao receber `type=message`:
  1) persistir `Message`
  2) **Celery task** `analyze_message_async(tenant_id, message_id, prompt_version=None, is_shadow=False, run_id='prod')`

## 🧠 IA via MCP (n8n HTTP) / HTTP local
- Configurar **N8N_AI_WEBHOOK** no `.env`.
- `apps/ai/tasks.py`:
  - montar payload:
    ```json
    {
      "tenant_id": "...",
      "message": "texto",
      "context": { "chat_id": "...", "sender": "..." },
      "prompt_version": "<versão ativa ou fornecida>"
    }
    ```
  - timeout + retries (3x) com backoff.
  - parse do JSON de resposta:
    ```json
    { "sentiment": 0.72, "emotion": "positivo", "satisfaction": 85, "tone": "...", "summary": "..." }
    ```
  - salvar no `Message` (se não shadow), registrar em `Inference`, gerar embedding e `write_embedding`.
- Expor também `/api/ai/analyze` (admin) p/ reprocessar uma mensagem.

## 🔁 Experimentos (replay & shadow)
- Endpoint admin `POST /api/experiments/replay` com `{ tenant_id, start, end, prompt_version, run_id }` → Celery `replay_window(...)` chamando `analyze_message_async(..., is_shadow=True)`.
- Em produção: `champion/challenger` (90/10) controlado por tabela `PromptTemplate` e feature flag:
  - O *campeão* preenche `Message`.
  - O *challenger* grava só em `Inference`.

## 🧾 Billing (Stripe)
- `billing.service`: criar/associar `stripe_customer_id`.
- Webhook Stripe: eventos de pagamento (invoice.paid/failed).
- Cron diário (Celery beat ou `cron` do sistema) para:
  - checar `next_billing_date` e criar cobrança
  - se falhar, `status = suspended` no tenant
- Planos e limites:
  - Starter: 1 número, retain 30d
  - Pro: 3 números, retain 180d
  - Scale: 6 números, retain 365d
  - Enterprise: custom
- Middleware/Service de verificação de limites (ex.: `connections.count <= plan_limit`).

## 🌐 API (DRF) — endpoints mínimos
- `POST /api/auth/login` | `GET /api/me`
- `GET /api/tenants/:id/metrics` (médias, volumes, p95 latência)
- `GET /api/messages?chat_id=&q=&page=...` (páginação + FTS)
- `POST /api/messages/semantic-search` { query } → top-K via pgvector
- `GET/POST /api/connections`
- `GET/POST /api/prompts` (registrar novas versões)
- `POST /api/experiments/replay` (admin)
- `POST /api/ai/analyze` (admin)
- `POST /api/webhooks/stripe`
- (Opcional) `POST /api/webhooks/evolution` como fallback HTTP

## 🔌 WebSockets (Channels)
- Rota: `ws/tenant/<tenant_id>/`
- Consumer agrupa por `tenant_<id>`.
- Após análise IA, emitir evento `message_analyzed` com `{ message_id, sentiment, satisfaction, emotion, tone }`.

## 🧰 Frontend (React)
- Páginas: Login, Dashboard, Conversas, Detalhe, Conexões, Experimentos, Billing
- Tailwind + shadcn para UI; Zustand para estado global
- Conectar WS: `ws://<backend>/ws/tenant/<tenant_id>/`
- Semantic search: chamar `/api/messages/semantic-search`
- Billing: página com plano atual, próximas cobranças, upgrade (Stripe Checkout/Portal)

## ⚙️ Configuração & Deploy (Railway)
- `.env.example`:
  ```
  DJANGO_SECRET_KEY=
  DATABASE_URL=postgresql://user:pass@host:port/db
  REDIS_URL=redis://host:port/0
  STRIPE_SECRET_KEY=
  N8N_AI_WEBHOOK=https://<n8n>/webhook/ai-analysis
  EVO_BASE_URL=
  ALLOWED_HOSTS=*
  ```
- Railway Postgres: rodar `CREATE EXTENSION IF NOT EXISTS vector;` uma única vez.
- Backend start: `python manage.py migrate && daphne -b 0.0.0.0 -p 8000 evosense.asgi:application`
- Celery: `celery -A evosense worker -l info` e (se usar beat) `celery -A evosense beat -l info`
- Frontend: `npm run build && npm run preview -- --host 0.0.0.0 --port 5173`

## 🧪 Qualidade & Observabilidade
- Linters (ruff/flake8), Black, isort.
- Logs estruturados (json) com request-id/tenant-id.
- Healthchecks: `/health` backend; WS ping/pong.
- Testes: unit de services/DAO e e2e básicos de API.

## 📄 Docs & Rules (gerar arquivos)
- `rules.md`: convenções de código, módulos, segurança, limites de plano, versionamento de prompt, políticas de PII.
- `docs/ARCHITECTURE.md`: diagrama, fluxos ingestão→IA→persistência→WS.
- `docs/DB_SCHEMA.md`: tabelas, chaves, índices (incluindo pgvector).
- `docs/DEPLOYMENT.md`: Railway (serviços, envs, comandos), migrações e inicialização.

## ✅ Entregáveis
1) Monorepo com **backend e frontend** prontos para build.
2) Migrations incluindo **pgvector**.
3) Ingestion WebSocket funcional (arquivo `ingestion/evolution_ws.py`) e Celery tasks de análise.
4) DAO pgvector (write/search).
5) Endpoints DRF e Consumer Channels mínimos.
6) Páginas React e hook WS.
7) Scripts de deploy Railway.
8) Seeds opcionais (um `PromptTemplate` default e criação de planos).

> Objetivo: **código pronto para subir no Railway**, com ingestão Evolution, IA via MCP/HTTP, busca semântica, multi-tenant, billing e experimentos de prompt.


se quiser, adapto esse prompt para “codegen direto” (o Cursor já sair escrevendo os arquivos), ou te entrego os arquivos iniciais (rules.md, docs/*.md, requirements.txt, docker-compose.yml) prontos pra colar. Quer que eu gere esses arquivos agora?

Você disse:
pode adpta e gera o rules.mf o restante é com ele
O ChatGPT disse:

Perfeito — aqui está o arquivo rules.md totalmente ajustado ao stack que você definiu:
Django + DRF + Channels + Celery + React + PostgreSQL + pgvector + Railway + Evolution API + MCP (n8n).
Esse arquivo serve como guia técnico, estilo de código e política de arquitetura para o Cursor seguir.

# 🚦 EVO SENSE — RULES OF DEVELOPMENT

## 🧭 Visão Geral
EvoSense é uma plataforma **SaaS multi-tenant** para análise de sentimento e satisfação de clientes em conversas do WhatsApp (Evolution API).  
Stack principal:
- **Backend:** Django 5 + DRF + Channels + Celery
- **Frontend:** React + TypeScript + Vite + Tailwind + shadcn/ui
- **Banco:** PostgreSQL + pgvector
- **Infra:** Docker + Railway
- **IA:** via MCP (n8n HTTP) ou modelo local (Qwen/Ollama)
- **Billing:** Stripe (30 dias)
- **Realtime:** Django Channels (WebSocket)
- **Experimentos:** versionamento de prompts e shadow inference

---

## 🧱 Arquitetura
- Estrutura de monorepo:


/backend
/apps
/tenancy
/authn
/connections
/messages
/ai
/billing
/experiments
/ingestion (Evolution WebSocket)
/common
/frontend

- Banco: **PostgreSQL 15+** (Railway)  
- Extensão `pgvector` habilitada: `CREATE EXTENSION IF NOT EXISTS vector;`
- Processos assíncronos com **Celery + Redis**.
- Eventos em tempo real via **Channels (Redis layer)**.
- Deploy via **Railway Docker service**.

---

## 🔐 Multi-tenancy e Segurança
- Cada modelo tem `tenant_id` (FK → Tenant).
- `User` customizado (extends `AbstractUser`) com `tenant` e `role (admin/operator)`.
- Todas as queries filtram por `tenant_id`.
- Dados sensíveis (tokens Evolution, Stripe) são criptografados ou armazenados em cofres.
- JWT/Session Auth padrão DRF.
- Sanitização de mensagens (regex para PII: CPF, email, cartão).
- WebSockets segregados por tenant: `/ws/tenant/<tenant_id>/`.

---

## 📡 Ingestão (Evolution API)
- Cada conexão ativa gera um listener WebSocket (`evo_ws_url`, `evo_token`).
- Ao receber uma mensagem:
1. Cria registro em `messages_message`.
2. Enfileira task Celery `analyze_message_async`.
3. Após análise IA, envia evento `message_analyzed` via Channels.

---

## 🧠 IA (MCP / Qwen / Ollama)
- Webhook HTTP configurável (`N8N_AI_WEBHOOK`).
- Payload:
```json
{
  "tenant_id": "...",
  "message": "...",
  "context": { "chat_id": "...", "sender": "..." },
  "prompt_version": "p_v1_base"
}


Retorno esperado:

{
  "sentiment": 0.72,
  "emotion": "positivo",
  "satisfaction": 85,
  "tone": "cordial",
  "summary": "Cliente satisfeito com a resposta."
}


Prompt versionado em PromptTemplate.

Experimentos de shadow inference (champion/challenger).

Embeddings gerados localmente (Qwen/Ollama) e armazenados em pgvector.

🗃️ Banco de Dados

Tabelas principais:

tenancy_tenant

authn_user

connections_evolutionconnection

messages_message

coluna embedding vector(768)

experiments_prompttemplate

experiments_inference

billing_paymentaccount

Índices

GIN em text (full-text).

IVFFLAT em embedding vector_cosine.

Index em created_at e chat_id.

🧩 IA Experiments

PromptTemplate define corpo do prompt e version.

Inference armazena resultados, latência e run_id.

Backfill (replay_window) reprocessa mensagens históricas com prompt novo.

Shadow inference: roda prompt challenger em paralelo (10% tráfego).

💳 Billing

Stripe SDK + webhooks.

PaymentAccount armazena stripe_customer_id e status.

Cron diário:

Cobra tenants com next_billing_date <= hoje.

Atualiza status (active, suspended).

Planos:

Plano	Conexões	Retenção	Preço (BRL)
Starter	1	30 dias	199
Pro	3	180 dias	499
Scale	6	365 dias	999
Enterprise	custom	2 anos	sob contrato
🔌 API (DRF)

Endpoints obrigatórios:

/api/auth/login, /api/me

/api/messages (listagem, filtros, full-text)

/api/messages/semantic-search (pgvector)

/api/connections (CRUD)

/api/prompts (versões)

/api/experiments/replay

/api/ai/analyze

/api/tenants/:id/metrics

/api/webhooks/stripe

/health

⚙️ Celery

Worker + Beat habilitados.

Filas:

default — tasks de IA

billing — cobranças

experiments — replays

Retries automáticos e logs estruturados (tenant_id, task_id, latency_ms).

🌐 WebSockets (Channels)

Consumer principal: TenantConsumer

Grupo: tenant_<id>

Eventos:

message_analyzed

connection_status

Broadcast de mensagens IA para frontend em tempo real.

💻 Frontend (React)

Login (JWT) + Painel principal.

Dashboard de KPIs:

média de satisfação, % positivas, mensagens/dia.

Conversas (lista + detalhe).

Busca semântica (input → /api/messages/semantic-search).

Aba de conexões Evolution.

Aba de experimentos (prompts + comparativos).

Billing (plano atual, cobrança Stripe).

WebSocket conectado via ws://<backend>/ws/tenant/<tenant_id>/.

🧰 Boas práticas

Código Python: formatar com Black + Ruff.

Código JS/TS: ESLint + Prettier.

Nomenclatura consistente (snake_case backend, camelCase frontend).

Módulos separados por domínio (clean architecture).

Padrão de commits: Conventional Commits.

Testes unitários e integração (pytest + DRF test client).

📄 Segurança

Criptografia de tokens Evolution e Stripe.

Sanitização de logs (nunca armazenar texto sensível completo).

Rate limiting nos endpoints públicos.

CORS configurado apenas para domínios do cliente.

HTTPS obrigatório em produção.

☁️ Railway Deploy

Serviços:

backend: Django + Channels + Celery worker.

redis: Redis 7.x.

postgres: Railway Postgres (extensão vector).

frontend: React build.

Comandos:

python manage.py migrate
celery -A evosense worker -l info
daphne -b 0.0.0.0 -p 8000 evosense.asgi:application
npm run build && npm run preview

🔎 Observabilidade

Logs estruturados JSON (pino ou loguru).

Métricas básicas: requisições, latência IA, volume de mensagens.

Endpoint /health e /metrics (Prometheus-style).

🧾 Versionamento de Prompt

Cada prompt tem version, hash e body.

Toda inferência armazena qual versão foi usada.

Permitir replay histórico com novas versões.

Guardar dataset de treino interno (text, sentiment, emotion, satisfaction) para futuras calibrações.

🧠 Estratégia de IA local

Modelo: Qwen (rodando on-prem ou Ollama).

Custo por mensagem: ~zero.

Quantização INT4/INT8 + batching.

Embeddings: Qwen-mini ou Ollama embedding model.

Repositório de experimentos versionado (prompt_vN).

✅ Regras finais

Nenhum dado cru de cliente fora do banco.

Cada Tenant isolado em queries e permissões.

Testar todos os endpoints críticos com JWT.

IA deve sempre retornar JSON estrito e validado.

pgvector usado apenas para busca, não para storage primário.

Só salvar texto bruto de mensagens se plano permitir.---
alwaysApply: true
---
